# -*- coding: utf-8 -*-
"""Redes Neuronales Recurrentes RNN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HKibY1E1aIQPQam0ZrhfA7vrZ_SoNrK4
"""

# CELDA 1: Configuración inicial
!pip install tensorflow nltk matplotlib scikit-learn pandas numpy

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import nltk
import re
import string
from unicodedata import normalize
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, SimpleRNN, LSTM, Dense, TimeDistributed
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import pandas as pd
from sklearn.model_selection import train_test_split
import json
import os
from google.colab import files

nltk.download('punkt')
print("Entorno listo!")

# === DESCARGA Y EXTRACCIÓN AUTOMÁTICA DE ARCHIVOS MONOLINGÜES ===
import urllib.request
import gzip
import shutil
import os
import urllib.request
import os

# REEMPLAZA ESTOS ENLACES CON TUS ENLACES REALES
ES_URL = "https://object.pouta.csc.fi/OPUS-QED/v2.0a/mono/es.txt.gz"      # ← TU LINK PARA ESPAÑOL
QUE_URL = "https://object.pouta.csc.fi/OPUS-QED/v2.0a/mono/que.txt.gz"    # ← TU LINK PARA QUECHUA

def descargar_y_extraer(url, nombre_salida):
    gz_path = f"{nombre_salida}.gz"
    txt_path = f"{nombre_salida}.txt"

    # Descargar archivo .gz
    print(f"Descargando {gz_path}...")
    urllib.request.urlretrieve(url, gz_path)
    print(f"{gz_path} descargado correctamente.")

    # Extraer archivo .gz
    print(f"Extrayendo {gz_path} a {txt_path}...")
    try:
        with gzip.open(gz_path, 'rb') as f_in:
            with open(txt_path, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        print(f"{txt_path} extraído correctamente.")
    except Exception as e:
        print(f"❌ Error al extraer {gz_path}: {e}")
        return

    # Eliminar el archivo .gz tras la extracción (opcional)
    os.remove(gz_path)
    print(f"{gz_path} eliminado.")

    # Verificar que el .txt existe
    assert os.path.exists(txt_path), f"¡Error! No se pudo crear {txt_path}"

# Procesar español y quechua
descargar_y_extraer(ES_URL, "es")
descargar_y_extraer(QUE_URL, "que")

print("✅ ¡Ambos archivos .txt descargados y extraídos correctamente!")

# === LIMPIEZA Y PREPROCESAMIENTO ===
import re
import string
from unicodedata import normalize

def clean_text(text, lang):
    # Normalización Unicode (crucial para quechua con acentos)
    text = normalize('NFC', text.strip())
    if not text:
        return ""

    # Minúsculas
    text = text.lower()

    # Eliminar puntuación excesiva (mantener guiones y apóstrofes si es necesario)
    punctuation = string.punctuation.replace("'", "").replace("-", "")  # opcional: mantener
    text = re.sub(f"[{re.escape(punctuation)}]", "", text)

    # Espacios múltiples → uno solo
    text = re.sub(r'\s+', ' ', text).strip()

    # Añadir tokens de inicio y fin
    if lang == 'es':
        text = '<start> ' + text + ' <end>'
    else:  # quechua
        text = '<start> ' + text + ' <end>'

    return text

# === CARGAR ARCHIVOS ===
with open('es.txt', 'r', encoding='utf-8') as f:
    es_lines = f.readlines()

with open('que.txt', 'r', encoding='utf-8') as f:
    qu_lines = f.readlines()

# Asegurarse de que tengan el mismo número de líneas
min_lines = min(len(es_lines), len(qu_lines))
print(f"Alineando {min_lines} pares de oraciones...")

es_lines = es_lines[:min_lines]
qu_lines = qu_lines[:min_lines]

# === LIMPIAR Y CREAR PARES ===
pairs = []
for es, qu in zip(es_lines, qu_lines):
    es_clean = clean_text(es, 'es')
    qu_clean = clean_text(qu, 'que')
    if es_clean.count(' ') > 3 and qu_clean.count(' ') > 3:  # filtrar oraciones muy cortas
        pairs.append((es_clean, qu_clean))

print(f"Total de pares válidos: {len(pairs)}")
print("Ejemplo:")
print("ES:", pairs[0][0])
print("QU:", pairs[0][1])

# CELDA 3: Tokenización
es_tokenizer = Tokenizer(filters='')
qu_tokenizer = Tokenizer(filters='')

es_tokenizer.fit_on_texts([p[0] for p in pairs])
qu_tokenizer.fit_on_texts([p[1] for p in pairs])

vocab_es = len(es_tokenizer.word_index) + 1
vocab_qu = len(qu_tokenizer.word_index) + 1

encoder_input = es_tokenizer.texts_to_sequences([p[0] for p in pairs])
decoder_input = qu_tokenizer.texts_to_sequences([p[1].replace('<end>', '') + ' <end>' for p in pairs])
decoder_target = qu_tokenizer.texts_to_sequences([p[1] for p in pairs])

max_len_es = max(len(seq) for seq in encoder_input)
max_len_qu = max(len(seq) for seq in decoder_target)

encoder_input = pad_sequences(encoder_input, maxlen=max_len_es, padding='post')
decoder_input = pad_sequences(decoder_input, maxlen=max_len_qu, padding='post')
decoder_target = pad_sequences(decoder_target, maxlen=max_len_qu, padding='post')

# Dividir datos
X_train_enc, X_test_enc, X_train_dec, X_test_dec, y_train, y_test = train_test_split(
    encoder_input, decoder_input, decoder_target, test_size=0.2, random_state=42)

print(f"Train: {len(X_train_enc)}, Test: {len(X_test_enc)}")
print(f"Vocab ES: {vocab_es}, Vocab QU: {vocab_qu}")
print(f"Max len ES: {max_len_es}, Max len QU: {max_len_qu}")

# CELDA 4: RNN Simple
encoder_inputs = Input(shape=(max_len_es,))
enc_emb = Embedding(vocab_es, 256, mask_zero=True)(encoder_inputs)
encoder_rnn = SimpleRNN(512, return_state=True)
encoder_outputs, state_h = encoder_rnn(enc_emb)
encoder_states = [state_h]

decoder_inputs = Input(shape=(max_len_qu,))
dec_emb_layer = Embedding(vocab_qu, 256, mask_zero=True)
dec_emb = dec_emb_layer(decoder_inputs)
decoder_rnn = SimpleRNN(512, return_sequences=True, return_state=True)
decoder_outputs, _ = decoder_rnn(dec_emb, initial_state=encoder_states)
decoder_dense = TimeDistributed(Dense(vocab_qu, activation='softmax'))
decoder_outputs = decoder_dense(decoder_outputs)

model_rnn = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model_rnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

print(model_rnn.summary())

# CELDA 5: Entrenamiento RNN
history_rnn = model_rnn.fit(
    [X_train_enc, X_train_dec], y_train[..., np.newaxis],
    validation_data=([X_test_enc, X_test_dec], y_test[..., np.newaxis]),
    batch_size=32,
    epochs=30,
    verbose=1
)

model_rnn.save('rnn_simple.h5')

# CELDA 6: Modelo LSTM
encoder_inputs_l = Input(shape=(max_len_es,))
enc_emb_l = Embedding(vocab_es, 256, mask_zero=True)(encoder_inputs_l)
encoder_lstm = LSTM(512, return_state=True)
encoder_outputs_l, state_h_l, state_c_l = encoder_lstm(enc_emb_l)
encoder_states_l = [state_h_l, state_c_l]

decoder_inputs_l = Input(shape=(max_len_qu,))
dec_emb_l = dec_emb_layer(decoder_inputs_l)
decoder_lstm = LSTM(512, return_sequences=True, return_state=True)
decoder_outputs_l, _, _ = decoder_lstm(dec_emb_l, initial_state=encoder_states_l)
decoder_outputs_l = decoder_dense(decoder_outputs_l)

model_lstm = Model([encoder_inputs_l, decoder_inputs_l], decoder_outputs_l)
model_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

print(model_lstm.summary())

# CELDA 7: Entrenamiento LSTM
history_lstm = model_lstm.fit(
    [X_train_enc, X_train_dec], y_train[..., np.newaxis],
    validation_data=([X_test_enc, X_test_dec], y_test[..., np.newaxis]),
    batch_size=32,
    epochs=30,
    verbose=1
)

model_lstm.save('lstm_model.h5')

# CELDA 8: Función de traducción
def translate(sentence, model, enc_tok, dec_tok, max_enc, max_dec):
    seq = enc_tok.texts_to_sequences([sentence])
    seq = pad_sequences(seq, maxlen=max_enc, padding='post')

    if 'SimpleRNN' in str(model.layers):
        states = model.layers[2].states[0]
        states = model.predict([seq, np.zeros((1, max_dec))], verbose=0)[1]
    else:
        states = [model.predict([seq, np.zeros((1, max_dec))], verbose=0)[1],
                  model.predict([seq, np.zeros((1, max_dec))], verbose=0)[2]]

    target_seq = np.zeros((1, max_dec))
    target_seq[0, 0] = dec_tok.word_index['<start>']

    output = []
    for i in range(1, max_dec):
        if 'SimpleRNN' in str(model.layers):
            preds, h = model.predict([seq, target_seq], verbose=0)
            states = [h]
        else:
            preds, h, c = model.predict([seq, target_seq], verbose=0)
            states = [h, c]
        idx = np.argmax(preds[0, i-1, :])
        if idx == dec_tok.word_index.get('<end>', 0):
            break
        word = dec_tok.index_word.get(idx, '')
        output.append(word)
        target_seq[0, i] = idx
    return ' '.join(output)

# BLEU
smoothie = SmoothingFunction().method4
bleu_rnn, bleu_lstm = [], []

test_pairs = [(es_tokenizer.sequences_to_texts([x])[0].replace('<start>', '').replace('<end>', '').strip(),
               qu_tokenizer.sequences_to_texts([y])[0].replace('<start>', '').replace('<end>', '').strip())
              for x, y in zip(X_test_enc[:50], y_test[:50])]

for es, qu in test_pairs:
    pred_rnn = translate(es, model_rnn, es_tokenizer, qu_tokenizer, max_len_es, max_len_qu)
    pred_lstm = translate(es, model_lstm, es_tokenizer, qu_tokenizer, max_len_es, max_len_qu)

    ref = qu.split()
    bleu_rnn.append(sentence_bleu([ref], pred_rnn.split(), smoothing_function=smoothie))
    bleu_lstm.append(sentence_bleu([ref], pred_lstm.split(), smoothing_function=smoothie))

print(f"BLEU RNN: {np.mean(bleu_rnn):.4f}")
print(f"BLEU LSTM: {np.mean(bleu_lstm):.4f}")

# CELDA 9: Gráficos
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(history_rnn.history['loss'], label='RNN Train')
plt.plot(history_rnn.history['val_loss'], label='RNN Val')
plt.plot(history_lstm.history['loss'], label='LSTM Train')
plt.plot(history_lstm.history['val_loss'], label='LSTM Val')
plt.title('Pérdida')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history_rnn.history['accuracy'], label='RNN Train')
plt.plot(history_rnn.history['val_accuracy'], label='RNN Val')
plt.plot(history_lstm.history['accuracy'], label='LSTM Train')
plt.plot(history_lstm.history['val_accuracy'], label='LSTM Val')
plt.title('Precisión')
plt.legend()
plt.show()

# CELDA 10: PRUEBA INTERACTIVA (RNN vs LSTM)

from IPython.display import clear_output
import time

def translate_interactive(sentence, model, enc_tok, dec_tok, max_enc, max_dec, model_name):
    seq = enc_tok.texts_to_sequences([sentence])
    seq = pad_sequences(seq, maxlen=max_enc, padding='post')

    # Obtener estados iniciales del encoder
    if 'SimpleRNN' in str(model.layers):
        _, state = model.layers[2](model.layers[1](seq))
        states = [state]
    else:
        _, state_h, state_c = model.layers[2](model.layers[1](seq))
        states = [state_h, state_c]

    target_seq = np.zeros((1, max_dec))
    target_seq[0, 0] = dec_tok.word_index.get('<start>', 1)

    output = []
    for i in range(1, max_dec):
        if 'SimpleRNN' in str(model.layers):
            preds, h = model.layers[-3](model.layers[-4](target_seq, initial_state=states))
            states = [h]
        else:
            preds, h, c = model.layers[-3](model.layers[-4](target_seq, initial_state=states))
            states = [h, c]

        idx = np.argmax(preds[0, i-1, :])
        if idx == dec_tok.word_index.get('<end>', 0):
            break
        word = dec_tok.index_word.get(idx, '')
        if word:
            output.append(word)
        target_seq[0, i] = idx

    return ' '.join(output)

print("TRADUCTOR ESPAÑOL → QUECHUA (RNN vs LSTM)")
print("Escribe 'salir' para terminar.\n")

while True:
    try:
        clear_output(wait=True)
        print("TRADUCTOR ESPAÑOL → QUECHUA")
        print("="*50)
        sentence = input("\nEspañol: ").strip()

        if sentence.lower() in ['salir', 'exit', 'quit']:
            print("¡Hasta luego!")
            break

        if not sentence:
            print("Por favor, escribe algo.")
            time.sleep(1)
            continue

        print("\nTraduciendo...")
        time.sleep(1)

        trad_rnn = translate_interactive(sentence, model_rnn, es_tokenizer, qu_tokenizer, max_len_es, max_len_qu, "RNN")
        trad_lstm = translate_interactive(sentence, model_lstm, es_tokenizer, qu_tokenizer, max_len_es, max_len_qu, "LSTM")

        print("\nRESULTADOS:")
        print(f"   RNN : {trad_rnn}")
        print(f"  LSTM : {trad_lstm}")
        print("\n" + "-"*50)
        input("Presiona Enter para traducir otra frase...")

    except Exception as e:
        print(f"Error: {e}")
        time.sleep(2)